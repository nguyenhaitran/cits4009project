---
title: "CITS4009-Individual Project"
author: "Henry TRAN - 23035141"
date: "2024-10-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Install all the required packages for data processing, data transformation and data visualization
library(ggplot2)
library(shiny)
library(gridExtra)
library(reshape2)
library(readxl)
library(dplyr)
```
In the world every year, there are different numbers of death recorded because of various reasons. Depends on the countries and regions, the numbers of death in each death reasons could be unique. For example, there are higher numbers of death recorded in high-income countries due to smoking, compared to low-income countries with higher numbers of death because of starving. In this report, the Country and Death causes dataset from World Health Organisation recorded from 1990 to 2019 is used for data exploration and modelling to investigate the death causes in different countries and how they will be utilised for making binary classifiers, clustering.

We need to load the data for this task
```{r}
# Read the data
path<-url("https://lms.uwa.edu.au/bbcswebdav/pid-3998436-dt-content-rid-47695412_1/courses/CITS4009_SEM-2_2024/Countries%20and%20death%20causes.csv")
```

# Data Exploratory
```{r}
data<-read.csv(path, header=TRUE)
head(data)
nrow(data)
ncol(data)
```
The data has 31 columns and 6840 rows. For the column names, they can be seen that the column names have dot-separated between each word.

```{r}
str(data)
```
```{r}
summary(data)
```
When exploring the data type of the dataset, Entity and Code are in character, while other columns are in numerical. As Entity and Code are unique for each country, then it should be changed to factor. Among the numerical data, Year should be changed to factor as well, since Year will not be used for any calculation.

```{r}
#converting Entity, Code and Year to factor
data$Entity<-as.factor(data$Entity)
data$Code<-as.factor(data$Code)
data$Year<-as.factor(data$Year)

summary(data)
str(data)
```
It could be seen that the each Entity and Code appeared 30 times in the dataset, which is matched with the number of years we have from 1990 to 2019. Among the numerical data, it could be seen that the gap between values are really large
```{r}
unique(data$Entity)
```
There are 228 unique entities but there are only 206 codes in this dataset. The reason is because there are some countries and regions do not have unique codes.

There are also no missing data

```{r}
#checking missing value
total_missing_value = sum(is.na(data))
total_missing_value
```

As I mentioned above, the gap between numbers of recorded death are extremely large within 28 death causes. The reason is because the dataset did not only record deaths of countries, but it also recorded the total deaths in groups, regions and in the world. Therefore, if we do the data analysis on the whole dataset, it will create skewness problem. The solution for this would be we can seperated data into different groups: countries and regions. When we do the boxplot of one death cause, it will be really skewed and we could not analyse anything from it

```{r}
#taking High.systolic.blood.pressure to illustrate the skewness
boxplot(data$High.systolic.blood.pressure)
```

## Data Transformation

Firstly, the data will be divided into 2 groups: countries and regions

There are 228 countries and regions in this data. There are few information we can know if the Entity is a region or world, including the Entity:
- Ends with (WB) or (WHO)
- Start with "World"
- G20
And anything other than this will be considered to be countries.
Here is how we will divide the data into groups

```{r}
#create a mask to filter out the regions and the regions
region_mask <- grepl("(WB)|(WHO)|^World|^G20|^OECD", data$Entity)

regions<-data[region_mask,]
countries<-data[!region_mask,]
```

```{r}
head(countries)
dim(countries)
```
With countries dataset, there are 6240 rows and 31 columns, including 28 death causes

```{r}
head(regions)
dim(regions)
```
While with regions dataset, there are 600 rows and 31 columns, also including 28 death causes

As the dataset is divided into 2 groups, then we will use both of them for the next steps of classification modelling and clustering.

#Model: Binary classification of High-income countries based on death causes
## Data Transformation
For the classification model, the model will classify the high-income countries based on the death causes. The reason to build this model is because the numbers of death in high-income countries could because of different causes compared to countries from other income-groups. For each death causes of each country, the total numbers of death will be calculated and then be used for classification and clustering.

According to World Bank, there are 86 countries in the high-income group. Therefore, we will have a column to label them, the country in high-income group will be 1 and 0 for countries from other income groups. 

There is a data file of countries in regions from World Bank. This file will be merge with the current data that we are processing

```{r}
#here is my file path, please change to your filepath to where you downloaded the file to run the code below
path<-"/Users/henrytran/Documents/UWA_Master-Data-Science/CITS4009/Project/"
```

```{r}
#create a path that can access and extractn the data
country_groups_file<- paste0(path, "World Bank Country Groups.xlsx")
country_groups_dataset<-read_excel(country_groups_file, sheet = "List of economies")
country_groups_dataset
```
The data that we need will include the the Economy, Code and Income group. Therefore, we will keep only 3 columns for this dataset

```{r}
country_groups_dataset<-country_groups_dataset[,c(1,2,4)] #We need the first 3 columns - Economy, Code, Region
country_groups_dataset
```
Now, we will merge countries dataset with country groups dataset using Code as key, and we keep all the data on the countries dataset
```{r}
countries_merged<-merge(countries, country_groups_dataset, by.x="Code", by.y="Code", all.x=TRUE)
head(countries_merged) 
```
As I look through both the dataset, there are 4 countries do not have Code, including Northen Ireland, Wales, Scotland, and England. They are all in United Kingdom (UK). Now we will double check to see if the total death of UK is the same to the total death of 4 countries
```{r}
#calculate the total death of UK over 30 years for all death causes
total_uk_death <- colSums(countries_merged[countries_merged$Entity=="United Kingdom", 4:31], na.rm = TRUE)
total_uk_death
```

```{r}
#calculate the total death of England, Northern Ireland, Wales, Scotland over 30 years for all death causes
four_countries_death <- subset(countries_merged, Entity %in% c("England", "Northern Ireland", "Wales", "Scotland"))
total_four_countries_death <- colSums(four_countries_death[, 4:31], na.rm = TRUE)
total_four_countries_death
```
After calculating the total deaths of 4 countries over years, the numbers of death recorded for each reasons are almost the same to the total deaths of UK over years. Therefore, we will drop these 4 countries out of the countries merged dataset.

Now we will calculate the total death of each country for all death causes.
```{r}
#we keep the remaining countries
countries_merged<-merge(countries, country_groups_dataset, by.x="Code", by.y="Code")
countries_merged<-countries_merged[,-32]
countries_merged
```
```{r}
#for the data, group the data into entity and income groups, then calculate the total death of each death causes of each country
names(countries_merged)[names(countries_merged) == "Income group"] <- "Income.group"
#using binning technique to calculate the sum of all death causes
total_countries_death <- countries_merged %>% group_by(Entity, Income.group) %>% summarise(across(c("Outdoor.air.pollution": "Iron.deficiency"), sum))
total_countries_death
```

After aggregating the model, now we will create a label for each country, in a column called 'Label'. As mentioned previously, the countries with high income will be 1 and 0 for countries in other income groups. Label will be the last column in this dataset

```{r}
label_total_countries_death<-total_countries_death
label_total_countries_death$Label<-ifelse(total_countries_death$Income.group == "High income", 1, 0)
label_total_countries_death$Label<-as.factor(label_total_countries_death$Label)
head(label_total_countries_death)
```
```{r}
#check if there is any NA values in the data
sum(is.na(label_total_countries_death))
label_total_countries_death[!complete.cases(label_total_countries_death),]
```
There is one country - Venezuela, which was not classified with income group. This is because it was not specified in the country_groups_dataset from World Bank. As the dataset has 201 rows and only one row has NA values, it is a good decision for us to remove this country
```{r}
label_total_countries_death <- na.omit(label_total_countries_death)
```

```{r}
label_total_countries_death
```
There are 200 countries, and 86 of the countries are in high-income group. Therefore, the ratio between number of countries in high and non-high incomes groups are close to equal.

##Building model
There are 2 models selected for high-income countries classification based on death causes. They are Decision Trees classifier, and Logistic Regression classifier

First of all, the data needs to be divided into 2 groups: training and testing.
```{r}
#we want to make the sampling be the same when we re-run it 
set.seed(12345)
random_mask<-runif(nrow(label_total_countries_death))<0.8 #80% of data will be used for training, 20% of data will be used for testing
train_data<-label_total_countries_death[random_mask,]
test_data<-label_total_countries_death[!random_mask,]
```
We only take the Label columns with 28 death causes. There are 152 rows and 29 columns for training dataset, and 48 rows and 29 columns for testing dataset
```{r}
train_data<-train_data[,3:ncol(train_data)]
train_data
cat("Training dataset size is", dim(train_data))
```
```{r}
test_data<-test_data[,3:ncol(test_data)]
test_data
cat("Testing dataset size is", dim(test_data))
```
```{r}
#we separated the columns, including the responses and all features
responses<- colnames(label_total_countries_death) %in% c("Entity", "Income.group", "Label") # create a mask to filter out the responses and the features 
all_features<-colnames(label_total_countries_death)[!responses]
all_features
```

### Best single variable and feature selections
#### Null model
In the training dataset, the probability of high-income countries in the dataset is around 0.322
```{r}
null_model_prob<-sum(train_data$Label==1)/nrow(train_data)
null_model_prob
```
The null model probability is 0.3223684. This means that there are there are around 32% of the countries in the training dataset is high-income countries. This implies that when we do the performance comparision, any model that we build needs to perform better than this baseline value.

#### Best single-variable model
Another way to check if our models' performance is to compare with best single-variate model. 

We will loop through all of the numerical variable to create single variate models with the training data. Then the best single variate model will be selected

Now we will run through all single numerical variables and calculate the area under the curve values (AUCs)
```{r}
#this is the code followed from the lecture, with some modification
mkPredC<-function(outCol, varCol, appCol){ #training outcomes, training variable and prediction variable
  #we convert all the data to vector for easier calculation
  outCol <- as.vector(outCol) 
  varCol <- as.vector(varCol)
  appCol <- as.vector(appCol)
  pOne<-sum(outCol==1)/length(outCol) #the probability of the outcome is 1 during training
  vTab<-table(outCol, varCol) #the table of training outcomes and training variable
  pOneWv<-(vTab[1,]+ 1.0e-3 *pOne)/(colSums(vTab)+ 1.0e-3) #how often the outcome is 1 with conditioned on levels of training variable
  pred<-pOneWv[appCol] #make prediction by looking up levels of appCol
  pred[is.na(pred)]<-pOne #add prediction for appCol that were unknown when training
  pred
}

mkPredN<- function(outCol, varCol, appCol){
  varCol <- as.numeric(varCol)  #we convert all the data to numerical to use for quantile function
  appCol <- as.numeric(appCol)  #we convert all the data to numerical to use for quantile function
  cuts<-unique(quantile(varCol, probs = seq(0,1,0.1), na.rm=T)) #we convert the numerical data to groups of categorical data
  varC<-cut(varCol, cuts) #we applied the cuts to the training variables
  appC<-cut(appCol, cuts) #we applied the cuts to the testing variable
  mkPredC(outCol, varC, appC)
}
```

```{r}
#this is the code followed from the lecture, with some modification
#import the ROCR library to create calcAUC function used for calculating single-variate model's AUCs 
library('ROCR')
calcAUC <- function(predcol,outcol) {
  perf <- performance(prediction(predcol,outcol==1),'auc') #we are checking the performance of model predicting the value 1
  as.numeric(perf@y.values)
}
```

```{r}
#this is the code followed from the lecture, with some modification
for(var in all_features) {
  pi <- paste('pred', var, sep='')#create the predFeatures columns for each Features
  #we ensure the input is vector for Label, numeric for all features
  train_data[,pi] <- mkPredN(train_data[["Label"]], as.numeric(unlist(train_data[[var]])), as.numeric(unlist(train_data[[var]])))
  test_data[,pi] <- mkPredN(test_data[["Label"]], as.numeric(unlist(test_data[[var]])),as.numeric(unlist(test_data[[var]])))
  #calculate the AUCs values of single-variate model with training and testing dataset
  aucTrain <- calcAUC(train_data[,pi], train_data[,"Label"])
  aucTest <- calcAUC(test_data[,pi], test_data[,"Label"])
  print(sprintf("%s: trainAUC: %4.3f; testAUC: %4.3f",pi, aucTrain, aucTest))
}
```
There are no variables having AUC higher than 0.5. This implies that there are no variables perform well in classifying the high-income countries with countries from other income groups. When calculating the area under the curve (AUC), it shows that the Secondhand.smoke variable has the highest AUC for training data, with value of 0.383. While looking at the AUC values for testing data, Smoking has the highest AUC with value of 0.318.

However, when try to predict whether a country is in non-high-income country group. The AUCs are so much higher on both training and testing data.
```{r}
#for this code, we are checking the performance of model predicting the value 0
calcAUC <- function(predcol,outcol) {
  perf <- performance(prediction(predcol,outcol==0),'auc')
  as.numeric(perf@y.values)
}
```

```{r}
#this is the code followed from the lecture, with some modification
for(var in all_features) {
  pi <- paste('pred', var, sep='') #create the predFeatures columns for each Features
  #we ensure the input is vector for Label, numeric for all features
  train_data[,pi] <- mkPredN(train_data[["Label"]], as.numeric(unlist(train_data[[var]])), as.numeric(unlist(train_data[[var]])))
  test_data[,pi] <- mkPredN(test_data[["Label"]], as.numeric(unlist(test_data[[var]])),as.numeric(unlist(test_data[[var]])))
  #calculate the AUCs values of single-variate model with training and testing dataset
  aucTrain <- calcAUC(train_data[,pi], train_data[,"Label"])
  aucTest <- calcAUC(test_data[,pi], test_data[,"Label"])
  print(sprintf("%s: trainAUC: %4.3f; testAUC: %4.3f",pi, aucTrain, aucTest))
}
```
Therefore, as the single variables are not good enough to predict the countries from high-income group based on death causes, we will use all the death cause variables for classification to see if we will have AUCs value better than 0.5

### Feature selection
The best feature is based on the largest log likelihood value compared
```{r}
#this is the code followed from the lecture, with some modification
# Create a function to calculate the log likelihood
logLikelihood<-function(ytrue, ypred, epsilon = 1e-15){
  ypred <- pmin(pmax(ypred, epsilon), 1 - epsilon)
  sum(ifelse(ytrue==1, log(ypred+epsilon), log(1-ypred-epsilon)), na.rm=T) #this is the formula to calculate the log-likelihood value
}
```

For the best single variable selection, we will calculate the log likelihood to select the best single variable to build the model.
Firstly, we will calculate the null log likelihood on the training dataset
```{r}
logNull<-logLikelihood(train_data[,"Label"], sum(train_data[,"Label"]==0)/nrow(train_data))
cat(logNull)
```

Now we will calculate the log likelihood for all variables
```{r}
#this is the code followed from the lecture, with some modification
one_num_var<-c()
minDrop<-10 # this is as a value to filter the data 
for (var in all_features){
  pi<-paste("pred", var, sep='') #create the predFeatures columns for each Features
  devDrop<-2*(logLikelihood(test_data[,"Label"], test_data[[pi]])-logNull)
  if (devDrop>=10){
    print(sprintf("%s, deviance reduction: %g", pi, devDrop))
    one_num_var<-c(one_num_var, var)
  }
}
```
```{r}
selected_features<-one_num_var
selected_features
```
Among all the vairables satisfied with the condition of log likelihood larger than 10, there are 15 features are good to classify the countries from high-income group based on death causes. They are:
- Outdoor.air.pollution   
- High.systolic.blood.pressure
- Diet.high.in.sodium
- Diet.low.in.whole.grains
- Alochol.use
- Diet.low.in.fruits
- Secondhand.smoke
- Unsafe.sex
- Diet.low.in.Vegetables
- Low.physical.activity
- Smoking
- High.fasting.plasma.glucose
- High.body.mass.index
- Drug.use
- Low.bone.mineral.density
Smoking is the model with the highest value of 185.363, slightly above the Diet low in Vegitables of 184.449. Therefore, we will use smoking as a single-variate model and use all 15 good classify features to build the decision tree classification model.

Before we continue, we will build a function to create the confusion matrix for both decision tree and logistic regression classifiers
```{r}
#this is the code followed from the lecture, with some modification
performanceMeasures <- function(pred, truth, name = "model") {
  # Construct confusion matrix ensuring all levels (0 and 1) are present
  ctable <- table(factor(truth, levels = c(0, 1)), factor(pred > 0.5, levels = c(FALSE, TRUE)))
  accuracy <- sum(diag(ctable)) / sum(ctable) #(TP+TN)/(TP+FP+TN+FN)
  precision <- ifelse(sum(ctable[, 2]) == 0, 0, ctable[2, 2] / sum(ctable[, 2])) #we prevent the division with 0, (TP)/(TP+FP)
  recall <- ifelse(sum(ctable[2, ]) == 0, 0, ctable[2, 2] / sum(ctable[2, ])) #we prevent the division with 0, (TP)/(TP+FN)
  f1 <- ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall)) #we prevent the division with 0, (2*precision*recall)/(precision+recall)
  data.frame(model = name, precision = precision, recall = recall, f1 = f1, accuracy = accuracy)
}

pretty_perf_table <- function(model,training,test, feature, model_type) {
  library(pander)
  # setting up Pander Options
  panderOptions("plain.ascii", TRUE)
  panderOptions("keep.trailing.zeros", TRUE)
  panderOptions("table.style", "simple")
  perf_justify <- "lrrrr"
  # comparing performance on training vs. test
  if (model_type == "logistic") {
    pred_train <- predict(model, newdata=training[, feature], type="response") #for logistic regression, we need to set predict type to "response"
    pred_test <- predict(model, newdata=test[, feature], type="response")
  } else if (model_type == "decision_tree") {
    pred_train <- predict(model, newdata=training[, feature], type="prob")[, 2]  # for logistic regression, we need to set predict type to "prob", and take the second column, as the result
    pred_test <- predict(model, newdata=test[, feature], type="prob")[, 2]
  }
  truth_train <- training[["Label"]]
  truth_test <- test[["Label"]]
  
  trainperf_tree <- performanceMeasures(pred_train, truth_train, "logistic, training")
  testperf_tree <- performanceMeasures(pred_test, truth_test, "logistic, test")
  
  perftable <- rbind(trainperf_tree, testperf_tree)
  pandoc.table(perftable, justify = perf_justify)
}
```

## Decision Tree Classification
We will build the decision tree classification, with all the features
### Full model
```{r}
library(rpart)
library(rpart.plot)
decision_full_model <- rpart(Label ~ ., data = train_data[, c(all_features, "Label")], method = "class",control = rpart.control(cp = 0.01))
rpart.plot(decision_full_model)
```

```{r}
#As we change the label of the previous part to 0, now we just change back to 1
calcAUC <- function(predcol,outcol) {
  perf <- performance(prediction(predcol,outcol==1),'auc')
  as.numeric(perf@y.values)
}
```

```{r}
print(calcAUC(predict(decision_full_model, train_data)[,2], train_data[,"Label"]))
```

```{r}
print(calcAUC(predict(decision_full_model, test_data)[,2], test_data[,"Label"]))
```
With full features, the decision tree classifiers perform much better than the single-variate model, with training AUC value of 0.928 andb AUC testing value of 0.872

```{r}
pretty_perf_table(decision_full_model, train_data, test_data, all_features, "decision_tree")
```
The model also so really good results for percision, recall, f1 and accurarcy

### Selected variables
As mentioned, there are 15 good classifier that we can use to classify the countries in high or non-high income groups. Therefore, we will use them to make the decision tree model.
```{r}
decision_multi_var_model <- rpart(Label ~ ., data = train_data[, c(selected_features, "Label")], method = "class")
rpart.plot(decision_multi_var_model)
```

```{r}
print(calcAUC(predict(decision_multi_var_model, train_data)[,2], train_data[,"Label"]))
```

```{r}
print(calcAUC(predict(decision_multi_var_model, test_data)[,2], test_data[,"Label"]))
```
With selected features, the decision tree classifiers perform closely to the full variables decision tree classifier better, around 0.01 different in AUC value. But with AUC testing value of 0.633, it is significantly lower thanfull variables decision tree classifier model's AUC of around 0.24. 
```{r}
pretty_perf_table(decision_multi_var_model, train_data, test_data, all_features, "decision_tree")
```
Based on the results of percision, recall, f1 and accurarcy, the selected variable decision tree classifier model did not performas good as the full variable model.

In short, based on AUC values, percision, recall, f1 and accurarcy calculated for 2 decision tree classifiers that we build, the model with all variables performs better than the 15-features-selected model. 

## Logistic Regression
We will build the logistic regression classification model, with all the features
### Full model
```{r}
formula<-paste("Label", paste(all_features, collapse=" + "), sep=" ~ ")
log_full_model<-glm(formula=formula, data=train_data, family=binomial(link="logit"))
train_data$log_full_pred <- predict(log_full_model, newdata=train_data, type="response")
test_data$log_full_pred <- predict(log_full_model, newdata=test_data, type="response")
```

```{r}
pretty_perf_table(log_full_model, train_data, test_data, all_features, "logistic")
```
With the logistic regression for all variables, the percision value is only around 0.55 to 0.61 for precision, implies that the model is still around randomly guessing the values. F1 and accurarcy values are around 0.71 to 0.74 when training and testing, which are good. Recall value is really high, with maximum for training and 0.9 for testing

```{r}
library(knitr)
kable(table(truth=test_data$Label, prediction=test_data$log_full_pred>0.5))
```
When we do the confusion matrix for the testing dataset, the model predict correctly 19 countries in high-income group, while only 2 wrong. However, the model did not well classify the countries from non-high income groups, there are 12 values predicted wrong to high-income country group. The investigation of the model will be demonstrate later.

### Selected feature
We will build the logistic regression classification model, with 15 selected features
```{r}
formula<-paste("Label", paste(selected_features, collapse=" + "), sep=" ~ ")
log_multi_var_model<-glm(formula=formula, data=train_data, family=binomial(link="logit"))
train_data$log_multi_var_pred <- predict(log_multi_var_model, newdata=train_data, type="response")
test_data$log_multi_var_pred <- predict(log_multi_var_model, newdata=test_data, type="response")
```

```{r}
pretty_perf_table(log_multi_var_model, train_data, test_data, all_features, "logistic")
```
Suprisingly, the logistic regression for 15-selected variables, the percision, f1 and accurarcy values are larger when comparing to the ones of full variable model in both training and testing, which are really good. However, recall value is smaller than full-feature classifier's recall, but still it signify a good classification model.

```{r}
kable(table(truth=test_data$Label, prediction=test_data$log_multi_var_pred>0.5))
```
When we do the confusion matrix for the testing dataset, the model predict correctly 17 countries in high-income group, while 4 wrong. However, the model did much better in classifying the countries from non-high income groups, there are 22 values predicted correctly and only 5 were incorrectly predicted to high-income country group. In overall, the selected-features logistic regression classification model performes better than the full variables one.

#### Additional ROC plots
With the ROC plots, we will compare how 2 models perform with the same set of features

We will start with 2 models with full features.
```{r}
#this is the code followed from the lecture, with some modification
library(ROCit)
# Function to plot ROC curves for both logistic regression and decision tree models
plot_roc <- function(predcol1_test, outcol1_test, predcol1_train, outcol1_train, 
                     predcol2_test, outcol2_test, predcol2_train, outcol2_train) {
  #we calcualte the ROC for the first model, which is the logistic regression model
  roc_1_test <- rocit(score = predcol1_test, class = outcol1_test == 1)
  roc_1_train <- rocit(score = predcol1_train, class = outcol1_train == 1)
  #we calcualte the ROC for the first model, which is the decision tree model
  roc_2_test <- rocit(score = predcol2_test, class = outcol2_test == 1)
  roc_2_train <- rocit(score = predcol2_train, class = outcol2_train == 1)
  # Plot test data for both models
  plot(roc_1_test, col = c("blue", "green"), lwd = 2, legend = FALSE, YIndex = FALSE, values = TRUE, asp = 1)
  lines(roc_2_test$TPR ~ roc_2_test$FPR, col = c("red", "green"), lwd = 2)
  lines(roc_1_train$TPR ~ roc_1_train$FPR, col = c("blue", "green"), lwd = 2, lty = 2)
  lines(roc_2_train$TPR ~ roc_2_train$FPR, col = c("red","green"), lwd = 2, lty = 2)
  #we add the legend to classify the training and testing lines for each model
  legend("bottomright", col = c("blue", "red","green"), 
         legend = c("Logistic Test", "Decision Tree Test", "Null Model", "Logistic Train", "Decision Tree Train"),
         lwd = 2,lty = c(1, 1, 2, 2, 2))
}
```

```{r, fig.asp=1}
#we generate predictions for both full-variables models on test and training datasets
#logistic regression model (using response)
pred_log_full_test <- predict(log_full_model, newdata = test_data, type = "response")
pred_log_full_train <- predict(log_full_model, newdata = train_data, type = "response")
#decision tree model (using probability)
pred_tree_full_test <- predict(decision_full_model, newdata = test_data, type = "prob")[, 2]
pred_tree_full_train <- predict(decision_full_model, newdata = train_data, type = "prob")[, 2]

plot_roc(pred_log_full_test, test_data$Label, pred_log_full_train, train_data$Label, pred_tree_full_test, test_data$Label, pred_tree_full_train, train_data$Label)

```
In the ROC plot for all features models, both the models perform better than the Null model. The decsion tree classifier's training and testing lines are closer to top left corner, in comparision to logistic regression classifier. Therefore, decsion tree classifier with full variables are better classifier. 

We will do the same thing with 2 models with selected features.
```{r, fig.asp=1}
#we generate predictions for both selected-features models on test and training datasets
#logistic regression model (using response)
pred_log_multi_test <- predict(log_multi_var_model, newdata = test_data, type = "response")
pred_log_multi_train <- predict(log_multi_var_model, newdata = train_data, type = "response")
#decision tree model (using probability)
pred_tree_multi_test <- predict(decision_multi_var_model, newdata = test_data, type = "prob")[, 2]
pred_tree_multi_train <- predict(decision_multi_var_model, newdata = train_data, type = "prob")[, 2]

plot_roc(pred_log_multi_test, test_data$Label, pred_log_multi_train, train_data$Label, pred_tree_multi_test, test_data$Label, pred_tree_multi_train, train_data$Label)

```
In the ROC plot for 15-selected features models, both models perform better than the Null model. The logistic regression classifier's training and testing lines are not a curve line, they are stepped. This could implies that the model is currently being overfitted or the dataset we are having is too small. Therefore, decsion tree classifier with 15-selected variables are better classifier. 

# Clustering
We will now working on clustering our data. For this one, the regions total death will be used. However, the column Income.group will not be included in this dataset, as it works as a label for the data. Therefore, the dataset will only include regions with total death records for each of the death cause.

First of all, we have to transform the data.

```{r}
regions
```
```{r}
total_regions_death <- regions %>% group_by(Entity) %>% summarise(across(c("Outdoor.air.pollution": "Iron.deficiency"), sum))
total_regions_death
```

```{r}
cluster_data<-total_regions_death
```

```{r}
cluster_data
```
We need to scale the data, with mean of 0 and standard deviation of 1 

```{r}
scale_data<-scale(cluster_data[, 2:ncol(cluster_data)]) # this will be the data where each column has 0 mean and unit standard deviation
```

```{r}
attr(scale_data, "scaled:scale")
```

```{r}
attr(scale_data, "scaled:center")
```
We will calculate the distance using euclidean distance, to form the vectors (or lines) of seperation between groups. 
```{r}
d<-dist(scale_data, method="euclidean")
d
```
```{r}
pfit<-hclust(d,method="ward.D2")
par(cex=0.5, mar=c(5, 5, 5, 5)) #we adjust the size of dendogram to be bigger and the labels be smaller
plot(pfit, labels = cluster_data$Entity, main="Cluster Dendogram for Death Causes") #we perform a hierarchial clustering
rect.hclust(pfit, k=3) #we will divide into 3 groups
```
```{r}
plot(as.dendrogram(pfit), xlim=c(1, 25))
```

This is the cutree function, which returns a vector of clustering group assigned to each row.
```{r}
groups <- cutree(pfit, k=3)
groups
```

Now we will visualise the clusters. To do that, we will perform principle components - dimension reduction, to visualise the cluster groups on 2 principal components (or 2D graph).

```{r}
princ <- prcomp(scale_data) #we calculate the principal components of scaled_df
nComp <- 2
project2D <- as.data.frame(predict(princ, newdata=scale_data)[,1:nComp]) #we put the scale data on the principal components dataframe
hclust.project2D <- cbind(project2D, cluster=as.factor(groups), country=cluster_data$Entity)
head(hclust.project2D)
```

```{r}
#this is the code from the lecture, with modification
#finding convex hull
library('grDevices')
#this function will return the vector of points in each convex hull
find_convex_hull <- function(proj2Ddf, groups) {
  do.call(rbind,lapply(unique(groups), FUN = function(c) {
    f <- subset(proj2Ddf, cluster==c);
    f[chull(f),] # this compute the convex hull of a set of points
    }))
  }
hclust.hull <- find_convex_hull(hclust.project2D, groups)
```


```{r}
#this is the code from lecture
ggplot(hclust.project2D, aes(x=PC1, y=PC2)) + geom_point(aes(shape=cluster, color=cluster)) + geom_text(aes(label=country, color=cluster), hjust=0, vjust=1, size=3) + geom_polygon(data=hclust.hull, aes(group=cluster, fill=as.factor(cluster)),alpha=0.4, linetype=0) + theme(text=element_text(size=20))
```
We can see that the data are divided into 3 groups, similar to the dendogram that we have above. 

We do not know which is the optimal number of k to divide the data into k groups. Therefore, we will find the number of cluster k.

There are 2 ways to find optimal k, either by looking for elbow curve when we calculate all k within sum of squares(WSS) or by calculating the Calinski-Harabasz index. In this paper work, the method of calculating WSS will be applied.

```{r}
#these are the code followed from the lecture, with some modificationl.
#this function will return the squared Euclidean distance of two given points x and y from the cluster centroid
sqr_euDist <- function(x, y) {
  sum((x - y)^2)
}
#this function calculates WSS of a cluster, represented as a n-by-d matrix (n is number of rows and d is the number of column) which contains only points of the cluster.
wss <- function(clustermat) {
  c0 <- colMeans(clustermat)
  sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} ))
}
#this function will calculate the total WSS.
wss_total <- function(scaled_df, labels) {
  wss.sum <- 0
  k <- length(unique(labels))
  for (i in 1:k)
    wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
    wss.sum
}
#this function will calculate the total sum of squared (TSS) distance of data points about the mean. This is the same as WSS when the k equals to 1.
tss <- function(scaled_df) {
  wss(scaled_df)
}
```

```{r}
#this is the code followed from the lecture, with modification
#this function will return the  indices computed using k-means clustering (`kmeans`) for a vector of k values ranging from 1 to kmax.
calculate_indices <- function(scaled_df, kmax) {
  npts <- nrow(scaled_df)
  wss.value <- numeric(kmax) # create a vector of numeric type
  # wss.value[1] stores the WSS value for k=1 (when all the
  # data points form 1 large cluster).
  wss.value[1] <- wss(scaled_df)
  d <- dist(scaled_df, method="euclidean")
  pfit <- hclust(d, method="ward.D2")
  for (k in 2:kmax) {
    labels <- cutree(pfit, k=k)
    wss.value[k] <- wss_total(scaled_df, labels)
  }
  bss.value <- tss(scaled_df) - wss.value # this is a vector
  B <- bss.value / (0:(kmax-1)) # also a vector
  W <- wss.value / (npts - 1:kmax) # also a vector
  data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
```

```{r}
library(gridExtra)
# calculate the CH criterion
crit.df <- calculate_indices(scale_data, 10)
ggplot(crit.df, aes(x=k, y=WSS), color="blue") + geom_point() + geom_line(colour="blue") + scale_x_continuous(breaks=1:10, labels=1:10) + theme(text=element_text(size=20))
```
When we look for the first 10 k values and the values for WSS, k equals to 2 is the best numbers of clusters.

## Shiny App
For data exploratory, it will be done using Shiny app
This will be removed

```{r}
logistic_model <- NULL
decision_model <- NULL
# Define UI for application that draws different plot
ui <- navbarPage(
    titlePanel("Visualisation of death in country/region during 1990 - 2019"),
    tabPanel("EDA",
    sidebarLayout(
        sidebarPanel(
          selectInput("plot_type", "Select plot type:", choices = c("Line plot", "Boxplot", "Heat map", "Correlation plot")),
            conditionalPanel(
              condition = "input.plot_type == 'Line plot'",
              checkboxInput("compare", "Compare between 2 countries/regions", FALSE),
            ),
            selectInput("region1", "Choose the first country/region:", choices = unique(data$Entity)),
            conditionalPanel(
              condition = "input.compare == true",
              selectInput("region2", "Choose the second country/region:", choices = unique(data$Entity))
            ),
            conditionalPanel(
              condition = "input.plot_type == 'Line plot'",
              selectInput("reason", "Choose death reason:", choices = names(data)[4:31])
            ),
        ),
        mainPanel(            
          plotOutput("plot1", width = "100%", height = "600px"),
            conditionalPanel(
                condition = "input.compare == true && input.plot_type == 'Line plot'",  # Display plot2 only if compare is checked
                plotOutput("plot2", width = "100%", height = "600px"))
        )
    )
),
# Tab 2: Decision Tree Classification Model
  tabPanel("Decision Tree Classification Model",
    sidebarLayout(
      sidebarPanel(
        radioButtons("tree_variable_set", "Select variable set:", choices = c("all_features", "selected_features"))
      ),
      mainPanel(
        plotOutput("decision_tree", width = "100%", height = "400px"),
        tableOutput("decision_tree_perf_table")
        
      )
    )
  ),
# Tab 3: Logistic Regression Model
  tabPanel("Logistic Regression Classification Model",
    sidebarLayout(
      sidebarPanel(
        radioButtons("logistic_variable_set", "Select variable set:", choices = c("all_features", "selected_features"))
      ),
      mainPanel(
        tableOutput("logistic_perf_table")
        
      )
    )
  ),
# Tab 4: Plot ROC
  tabPanel("Plots to compare AUC",
    sidebarLayout(
      sidebarPanel(
        radioButtons("variable_set", "Select variable set:", choices = c("all_features", "selected_features"))
      ),
      mainPanel(
        plotOutput("ROC_plot", width = "100%", height = "400px"),
      )
    )
  ),
# Tab 5: Clustering
  tabPanel("Clustering",
    sidebarLayout(
      sidebarPanel(
          selectInput("clustering_plot_type", "Select plot type:", choices = c("Dendogram", "Cluster Visualisation","WSS plot"))
      ),
      mainPanel(
        plotOutput("clustering_plot", width = "100%", height = "400px"),
      )
),
),
)


# Define the server
server <- function(input, output){
    # Render the histograms
    output$plot1 <- renderPlot({
      region_data<-data[data$Entity==input$region1,]
      # Plot selected death reason
      if (input$plot_type == 'Line plot'){
      ggplot(region_data, aes_string(x="Year", y=input$reason, group = "Entity")) +
          geom_line(color="darkorange") + 
          labs(x="Year", y = input$reason) +
          theme(axis.text.x = element_text(angle = 45, hjust = 1))}
      # Plot the boxplot
      else if (input$plot_type == 'Boxplot'){
        data_melt <- melt(region_data, id.vars = "Year", measure.vars = names(data)[4:31], 
                          variable.name = "death_reason", value.name = "value")
        ggplot(data_melt, aes(x = death_reason, y = value, fill = death_reason)) +
          geom_boxplot() +
          labs(x = "Death Reason", y = "Number of Deaths", title = paste("Boxplot of Death Reasons in", input$region1)) +
          theme_minimal() +
          theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "top")+
          guides(fill = guide_legend(ncol = 3))
        }
      
      # Plot the Heat Map for selected country
      else if (input$plot_type == 'Heat map'){
        data_melt <- melt(region_data, id.vars = "Year", measure.vars = names(data)[4:31], variable.name = "death_reason", value.name = "value")
        # Create heat map
        ggplot(data_melt, aes(x = Year, y = death_reason, fill = value)) + 
            geom_tile() + 
            scale_fill_gradient(low = "aliceblue", high = "blue") +
            scale_x_discrete(breaks = levels(region_data$Year)) +
            labs(x = "Year", y = "Death Reason", title = paste("Heat Map of Death Reasons in", input$region1)) +
            theme_minimal() +
            theme(axis.text.x = element_text(angle = 45, hjust = 1))
      }
      else if (input$plot_type == "Correlation plot"){
        # Extract the number of death for each death reasons
        numeric_data <- region_data[,4:31]
         
        # Calculate the correlation for all variables
        correlation <- cor(numeric_data)
        
        # Melt the correlation for visualization
        correlation_melted <- melt(correlation)
        
        # Create the correlation heatmap
        ggplot(correlation_melted, aes(x = Var1, y = Var2, fill = value)) +
          geom_tile() +
          scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0, limit = c(-1, 1), name = "Correlation") +
          theme_minimal() +
          theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
          labs(title = "Correlation Plot of Death Reasons", x = "Variables", y = "Variables")
      }
    })
    output$plot2 <- renderPlot({ 
      if (input$compare == TRUE && input$plot_type == 'Line plot'){
      region_data<-data[data$Entity==input$region2,]
      # Plot all death reasons
      ggplot(region_data, aes_string(x="Year", y=input$reason, group ="Entity")) + geom_line(color="darkolivegreen") + labs(x="Year", y = input$reason)
    }
  })
  # Decision Tree classification model
  output$decision_tree <- renderPlot({
    variable_set <- input$tree_variable_set
    if (variable_set == "all_features") {
      decision_model <- rpart(Label ~ ., data = train_data[, c(all_features, "Label")], method = "class",control = rpart.control(cp = 0.01))
    } else {
      decision_model <- rpart(Label ~ ., data = train_data[, c(selected_features, "Label")], method = "class")
    }
    rpart.plot(decision_model)
  })
  output$decision_tree_perf_table <- renderTable({
  # Get predictions from the decision tree model
    variable_set <- input$tree_variable_set
    if (variable_set == "all_features") {
      decision_model <- rpart(Label ~ ., data = train_data[, c(all_features, "Label")], method = "class",control = rpart.control(cp = 0.01))
    } else {
      decision_model <- rpart(Label ~ ., data = train_data[, c(selected_features, "Label")], method = "class")
    }
    table<-pretty_perf_table(decision_model, train_data, test_data, all_features, "decision_tree")
    table
})
  # 
  output$logistic_perf_table <- renderTable({
  # Get predictions from the logistic regression model
    variable_set <- input$logistic_variable_set
    if (variable_set == "all_features") {
      formula<-paste("Label", paste(all_features, collapse=" + "), sep=" ~ ")
      logistic_model<-glm(formula=formula, data=train_data, family=binomial(link="logit"))
    } else {
      formula<-paste("Label", paste(selected_features, collapse=" + "), sep=" ~ ")
      logistic_model <- glm(formula=formula, data=train_data, family=binomial(link="logit"))
    }
    table<-pretty_perf_table(logistic_model, train_data, test_data, all_features, "logistic")
    table
})
  #ROC plot
  output$ROC_plot <- renderPlot({
  # Get predictions from the logistic regression model
    variable_set <- input$variable_set
    if (variable_set == "all_features") {
      formula<-paste("Label", paste(all_features, collapse=" + "), sep=" ~ ")
      logistic_model<-glm(formula=formula, data=train_data, family=binomial(link="logit"))
      pred_log_test <- predict(logistic_model, newdata = test_data, type = "response")
      pred_log_train <- predict(logistic_model, newdata = train_data, type = "response")
      #decision tree model (using probability)
      decision_model <- rpart(Label ~ ., data = train_data[, c(all_features, "Label")], method = "class",control = rpart.control(cp = 0.01))
      pred_tree_test <- predict(decision_model, newdata = test_data, type = "prob")[, 2]
      pred_tree_train <- predict(decision_model, newdata = train_data, type = "prob")[, 2]
      
      plot_roc(pred_log_test, test_data$Label, pred_log_train, train_data$Label, pred_tree_test, test_data$Label, pred_tree_train, train_data$Label)
    } else {
      formula<-paste("Label", paste(selected_features, collapse=" + "), sep=" ~ ")
      logistic_model<-glm(formula=formula, data=train_data, family=binomial(link="logit"))
      pred_log_test <- predict(logistic_model, newdata = test_data, type = "response")
      pred_log_train <- predict(logistic_model, newdata = train_data, type = "response")
      #decision tree model (using probability)
      decision_model <- rpart(Label ~ ., data = train_data[, c(selected_features, "Label")], method = "class",control = rpart.control(cp = 0.01))
      pred_tree_test <- predict(decision_model, newdata = test_data, type = "prob")[, 2]
      pred_tree_train <- predict(decision_model, newdata = train_data, type = "prob")[, 2]
      
      plot_roc(pred_log_test, test_data$Label, pred_log_train, train_data$Label, pred_tree_test, test_data$Label, pred_tree_train, train_data$Label)
    }
})
  #clustering
  output$clustering_plot <- renderPlot({
    clustering_plot_type <- input$clustering_plot_type
    d<-dist(scale_data, method="euclidean")
    scale_data<-scale(cluster_data[, 2:ncol(cluster_data)])
    pfit<-hclust(d,method="ward.D2")
    if (clustering_plot_type == "Dendogram") {
      par(cex=0.5, mar=c(5, 5, 5, 5)) #we adjust the size of dendogram to be bigger and the labels be smaller
      plot(pfit, labels = cluster_data$Entity, main="Cluster Dendogram for Death Causes") #we perform a hierarchial clustering
      rect.hclust(pfit, k=3) #we will divide into 3 groups
    } else if (clustering_plot_type == "Cluster Visualisation") {
      groups <- cutree(pfit, k=3)
      
      princ <- prcomp(scale_data) #we calculate the principal components of scaled_df
      nComp <- 2
      project2D <- as.data.frame(predict(princ, newdata=scale_data)[,1:nComp]) #we put the scale data on the principal components dataframe
      hclust.project2D <- cbind(project2D, cluster=as.factor(groups), country=cluster_data$Entity)
      
      hclust.hull <- find_convex_hull(hclust.project2D, groups)
      
      ggplot(hclust.project2D, aes(x=PC1, y=PC2)) + geom_point(aes(shape=cluster, color=cluster)) + geom_text(aes(label=country, color=cluster), hjust=0, vjust=1, size=3) + geom_polygon(data=hclust.hull, aes(group=cluster, fill=as.factor(cluster)),alpha=0.4, linetype=0) + theme(text=element_text(size=20))
    } else{ #WSS plot
      crit.df <- calculate_indices(scale_data, 10)
      ggplot(crit.df, aes(x=k, y=WSS), color="blue") + geom_point() + geom_line(colour="blue") + scale_x_continuous(breaks=1:10, labels=1:10) +     theme(text=element_text(size=20))
    }
  })
}

# Run the Shiny app
shinyApp(ui = ui, server = server)
```















