---
title: "CITS4009-Individual Project"
author: "Henry TRAN - 23035141"
date: "2024-10-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Install all the required packages for data processing, data transformation and data visualization
library(ggplot2)
library(shiny)
library(gridExtra)
library(reshape2)
library(readxl)
library(dplyr)
```
In the world every year, there are different numbers of death recorded because of various reasons. Depends on the countries and regions, the numbers of death in each death reasons could be unique. For example, there are higher numbers of death recorded in high-income countries due to smoking, compared to low-income countries with higher numbers of death because of starving. In this report, the Country and Death causes dataset from World Health Organisation recorded from 1990 to 2019 is used for data exploration and modelling to investigate the death causes in different countries and how they will be utilised for making binary classifiers, clustering.

We need to load the data for this task
```{r}
# Read the data
path<-url("https://lms.uwa.edu.au/bbcswebdav/pid-3998436-dt-content-rid-47695412_1/courses/CITS4009_SEM-2_2024/Countries%20and%20death%20causes.csv")
```

# Data Exploratory
```{r}
data<-read.csv(path, header=TRUE)
head(data)
nrow(data)
ncol(data)
```
The data has 31 columns and 6840 rows. For the column names, they can be seen that the column names have dot-separated between each word.

```{r}
str(data)
```
```{r}
summary(data)
```
When exploring the data type of the dataset, Entity and Code are in character, while other columns are in numerical. As Entity and Code are unique for each country, then it should be changed to factor. Among the numerical data, Year should be changed to factor as well, since Year will not be used for any calculation.

```{r}
#converting Entity, Code and Year to factor
data$Entity<-as.factor(data$Entity)
data$Code<-as.factor(data$Code)
data$Year<-as.factor(data$Year)

summary(data)
str(data)
```
It could be seen that the each Entity and Code appeared 30 times in the dataset, which is matched with the number of years we have from 1990 to 2019. Among the numerical data, it could be seen that the gap between values are really large
```{r}
unique(data$Entity)
```
There are 228 unique entities but there are only 206 codes in this dataset. The reason is because there are some countries and regions do not have unique codes.

There are also no missing data

```{r}
#checking missing value
total_missing_value = sum(is.na(data))
total_missing_value
```

As I mentioned above, the gap between numbers of recorded death are extremely large within 28 death causes. The reason is because the dataset did not only record deaths of countries, but it also recorded the total deaths in groups, regions and in the world. Therefore, if we do the data analysis on the whole dataset, it will create skewness problem. The solution for this would be we can seperated data into different groups: countries and regions. When we do the boxplot of one death cause, it will be really skewed and we could not analyse anything from it

```{r}
#taking High.systolic.blood.pressure to illustrate the skewness
boxplot(data$High.systolic.blood.pressure)
```

## Data Transformation

Firstly, the data will be divided into 2 groups: countries and regions

There are 228 countries and regions in this data. There are few information we can know if the Entity is a region or world, including the Entity:
- Ends with (WB) or (WHO)
- Start with "World"
- G20
And anything other than this will be considered to be countries.
Here is how we will divide the data into groups

```{r}
#create a mask to filter out the regions and the regions
region_mask <- grepl("(WB)|(WHO)|^World|^G20|^OECD", data$Entity)

regions<-data[region_mask,]
countries<-data[!region_mask,]
```

```{r}
head(countries)
dim(countries)
```
With countries dataset, there are 6240 rows and 31 columns, including 28 death causes

```{r}
head(regions)
dim(regions)
```
While with regions dataset, there are 600 rows and 31 columns, also including 28 death causes

As the dataset is divided into 2 groups, then we will use both of them for the next steps of classification modelling and clustering.

#Model: Binary classification of High-income countries based on death causes
## Data Transformation
For the classification model, the model will classify the high-income countries based on the death causes. The reason to build this model is because the numbers of death in high-income countries could because of different causes compared to countries from other income-groups. For each death causes of each country, the total numbers of death will be calculated and then be used for classification and clustering.

According to World Bank, there are 86 countries in the high-income group. Therefore, we will have a column to label them, the country in high-income group will be 1 and 0 for countries from other income groups. 

There is a data file of countries in regions from World Bank. This file will be merge with the current data that we are processing

```{r}
#here is my file path, please change to your filepath to where you downloaded the file to run the code below
path<-"/Users/henrytran/Documents/UWA_Master-Data-Science/CITS4009/Project/"
```

```{r}
#create a path that can access and extractn the data
country_groups_file<- paste0(path, "World Bank Country Groups.xlsx")
country_groups_dataset<-read_excel(country_groups_file, sheet = "List of economies")
country_groups_dataset
```
The data that we need will include the the Economy, Code and Income group. Therefore, we will keep only 3 columns for this dataset

```{r}
country_groups_dataset<-country_groups_dataset[,c(1,2,4)] #We need the first 3 columns - Economy, Code, Region
country_groups_dataset
```
Now, we will merge countries dataset with country groups dataset using Code as key, and we keep all the data on the countries dataset
```{r}
countries_merged<-merge(countries, country_groups_dataset, by.x="Code", by.y="Code", all.x=TRUE)
head(countries_merged) 
```
As I look through both the dataset, there are 4 countries do not have Code, including Northen Ireland, Wales, Scotland, and England. They are all in United Kingdom (UK). Now we will double check to see if the total death of UK is the same to the total death of 4 countries
```{r}
#calculate the total death of UK over 30 years for all death causes
total_uk_death <- colSums(countries_merged[countries_merged$Entity=="United Kingdom", 4:31], na.rm = TRUE)
total_uk_death
```

```{r}
#calculate the total death of England, Northern Ireland, Wales, Scotland over 30 years for all death causes
four_countries_death <- subset(countries_merged, Entity %in% c("England", "Northern Ireland", "Wales", "Scotland"))
total_four_countries_death <- colSums(four_countries_death[, 4:31], na.rm = TRUE)
total_four_countries_death
```
After calculating the total deaths of 4 countries over years, the numbers of death recorded for each reasons are almost the same to the total deaths of UK over years. Therefore, we will drop these 4 countries out of the countries merged dataset.

Now we will calculate the total death of each country for all death causes.
```{r}
#we keep the remaining countries
countries_merged<-merge(countries, country_groups_dataset, by.x="Code", by.y="Code")
countries_merged<-countries_merged[,-32]
countries_merged
```
```{r}
#for the data, group the data into entity and income groups, then calculate the total death of each death causes of each country
names(countries_merged)[names(countries_merged) == "Income group"] <- "Income.group"
#using binning technique to calculate the sum of all death causes
total_countries_death <- countries_merged %>% group_by(Entity, Income.group) %>% summarise(across(c("Outdoor.air.pollution": "Iron.deficiency"), sum))
total_countries_death
```

After aggregating the model, now we will create a label for each country, in a column called 'Label'. As mentioned previously, the countries with high income will be 1 and 0 for countries in other income groups. Label will be the last column in this dataset

```{r}
label_total_countries_death<-total_countries_death
label_total_countries_death$Label<-ifelse(total_countries_death$Income.group == "High income", 1, 0)
label_total_countries_death$Label<-as.factor(label_total_countries_death$Label)
head(label_total_countries_death)
```
```{r}
#check if there is any NA values in the data
sum(is.na(label_total_countries_death))
label_total_countries_death[!complete.cases(label_total_countries_death),]
```
There is one country - Venezuela, which was not classified with income group. This is because it was not specified in the country_groups_dataset from World Bank. As the dataset has 201 rows and only one row has NA values, it is a good decision for us to remove this country
```{r}
label_total_countries_death <- na.omit(label_total_countries_death)
```

```{r}
label_total_countries_death
```
There are 200 countries, and 86 of the countries are in high-income group. Therefore, the ratio between number of countries in high and non-high incomes groups are close to equal.

##Building model
There are 2 models selected for high-income countries classification based on death causes. They are Decision Trees classifier, and Logistic Regression classifier

First of all, the data needs to be divided into 2 groups: training and testing.
```{r}
#we want to make the sampling be the same when we re-run it 
set.seed(12345)
random_mask<-runif(nrow(label_total_countries_death))<0.8 #80% of data will be used for training, 20% of data will be used for testing
train_data<-label_total_countries_death[random_mask,]
test_data<-label_total_countries_death[!random_mask,]
```
We only take the Label columns with 28 death causes. There are 152 rows and 29 columns for training dataset, and 48 rows and 29 columns for testing dataset
```{r}
train_data<-train_data[,3:ncol(train_data)]
train_data
cat("Training dataset size is", dim(train_data))
```
```{r}
test_data<-test_data[,3:ncol(test_data)]
test_data
cat("Testing dataset size is", dim(test_data))
```
```{r}
#we separated the columns, including the responses and all features
responses<- colnames(label_total_countries_death) %in% c("Entity", "Income.group", "Label") # create a mask to filter out the responses and the features 
all_features<-colnames(label_total_countries_death)[!responses]
all_features
```

### Best single variable and feature selections
#### Null model
In the training dataset, the probability of high-income countries in the dataset is 
```{r}
null_model_prob<-sum(train_data$Label==1)/nrow(train_data)
null_model_prob
```
The null model probability is 0.3223684. This means that there are there are around 32% of the countries in the training dataset is high-income countries. This implies that the model needs to perform better than this baseline value.

### Best single-variable model
We will loop through all of the numerical variable to create single variate models with the training data. Then the best single variate model will be selected

Now we will run through all single numerical variables and calculate the area under the curve values (AUCs)
```{r}
mkPredC<-function(outCol, varCol, appCol){ #training outcomes, training variable and prediction variable
  outCol <- as.vector(outCol)
  varCol <- as.vector(varCol)
  appCol <- as.vector(appCol)
  pOne<-sum(outCol==1)/length(outCol) #how often the outcome is 1 during training
  vTab<-table(as.factor(outCol), varCol)
  pOneWv<-(vTab[1,]+ 1.0e-3 *pOne)/(colSums(vTab)+ 1.0e-3) #how often the outcome is 1 with conditioned on levels of training variable
  pred<-pOneWv[appCol] #make prediction by looking up levels of appCol
  pred[is.na(pred)]<-pOne #add prediction for appCol that were unknown when training
  pred
}

mkPredN<- function(outCol, varCol, appCol){
  varCol <- as.numeric(varCol)  # Ensure it's numeric
  appCol <- as.numeric(appCol)  # Ensure it's numeric
  cuts<-unique(quantile(varCol, probs = seq(0,1,0.1), na.rm=T))
  varC<-cut(varCol, cuts)
  appC<-cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}
```

```{r}
library('ROCR')
calcAUC <- function(predcol,outcol) {
  perf <- performance(prediction(predcol,outcol==1),'auc')
  as.numeric(perf@y.values)
}
```

```{r}
for(var in all_features) {
  pi <- paste('pred', var, sep='')
  train_data[,pi] <- mkPredN(train_data[["Label"]], as.numeric(unlist(train_data[[var]])), as.numeric(unlist(train_data[[var]])))
  test_data[,pi] <- mkPredN(test_data[["Label"]], as.numeric(unlist(test_data[[var]])),as.numeric(unlist(test_data[[var]])))
  aucTrain <- calcAUC(train_data[,pi], train_data[,"Label"])
  aucTest <- calcAUC(test_data[,pi], test_data[,"Label"])
  print(sprintf("%s: trainAUC: %4.3f; testAUC: %4.3f",pi, aucTrain, aucTest))
}
```
There are no variables having AUC higher than 0.5. This implies that there are no variables perform well in classifying the high-income countries with countries from other income groups. When calculating the area under the curve (AUC), it shows that the Secondhand.smoke variable has the highest AUC for training data, with value of 0.383. While looking at the AUC values for testing data, Smoking has the highest AUC with value of 0.318.

However, when try to predict whether a country is in non-high-income country group. The AUCs are so much higher on both training and testing data.
```{r}
library('ROCR')
calcAUC <- function(predcol,outcol) {
  perf <- performance(prediction(predcol,outcol==0),'auc')
  as.numeric(perf@y.values)
}
```

```{r}
for(var in all_features) {
  pi <- paste('pred', var, sep='')
  train_data[,pi] <- mkPredN(train_data[["Label"]], as.numeric(unlist(train_data[[var]])), as.numeric(unlist(train_data[[var]])))
  test_data[,pi] <- mkPredN(test_data[["Label"]], as.numeric(unlist(test_data[[var]])),as.numeric(unlist(test_data[[var]])))
  aucTrain <- calcAUC(train_data[,pi], train_data[,"Label"])
  aucTest <- calcAUC(test_data[,pi], test_data[,"Label"])
  print(sprintf("%s: trainAUC: %4.3f; testAUC: %4.3f",pi, aucTrain, aucTest))
}
```
Therefore, as the single variables are not good enough to predict the countries from high-income group based on death causes, we will use all the death cause variables for classification.

### Feature selection
The best feature is based on the largest log likelihood value
```{r}
# Create a function to calculate the log likelihood
logLikelihood<-function(ytrue, ypred, epsilon = 1e-15){
  ypred <- pmin(pmax(ypred, epsilon), 1 - epsilon)
  sum(ifelse(ytrue==1, log(ypred+epsilon), log(1-ypred-epsilon)), na.rm=T)
}
```

For the best single variable selection, we will calculate the log likelihood to select the best single variable to build the model.
Firstly, we will calculate the null log likelihood on the training dataset
```{r}
logNull<-logLikelihood(train_data[,"Label"], sum(train_data[,"Label"]==0)/nrow(train_data))
cat(logNull)
```

Now we will calculate the log likelihood for all variables
```{r}
one_num_var<-c()
minDrop<-10 # this is as a value to filter the data
for (var in all_features){
  pi<-paste("pred", var, sep='')
  
  devDrop<-2*(logLikelihood(test_data[,"Label"], test_data[[pi]])-logNull)
  if (devDrop>=10){
    print(sprintf("%s, deviance reduction: %g", pi, devDrop))
    one_num_var<-c(one_num_var, var)
  }
}
```
```{r}
one_num_var
```

Among all the vairables satisfied with the condition of log likelihood larger than 10, there are 15 features are good to classify the countries from high-income group based on death causes. Smoking is the model with the highest value of 185.363, slightly above the Diet low in Vegitables of 184.449. Therefore, we will use smoking as a variable to build the decision tree classification model.

## Decision Tree Classification
We will build a function to create the confusion matrix for both decision tree and logistic regression classifiers
```{r}
performanceMeasures <- function(pred, truth, name = "model") {
  # Construct confusion matrix ensuring all levels (0 and 1) are present
  ctable <- table(factor(truth, levels = c(0, 1)), factor(pred > 0.5, levels = c(FALSE, TRUE)))
  accuracy <- sum(diag(ctable)) / sum(ctable)
  precision <- ifelse(sum(ctable[, 2]) == 0, 0, ctable[2, 2] / sum(ctable[, 2]))  # Prevent division by 0
  recall <- ifelse(sum(ctable[2, ]) == 0, 0, ctable[2, 2] / sum(ctable[2, ]))  # Prevent division by 0
  f1 <- ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))  # Handle case where both are 0
  data.frame(model = name, precision = precision, recall = recall, f1 = f1, accuracy = accuracy)
}

pretty_perf_table <- function(model,training,test, feature, model_type) {
  library(pander)
  # setting up Pander Options
  panderOptions("plain.ascii", TRUE)
  panderOptions("keep.trailing.zeros", TRUE)
  panderOptions("table.style", "simple")
  perf_justify <- "lrrrr"
  # comparing performance on training vs. test
  if (model_type == "logistic") {
    pred_train <- predict(model, newdata=training[, feature], type="response")
    pred_test <- predict(model, newdata=test[, feature], type="response")
  } else if (model_type == "decision_tree") {
    # For decision trees, we use the 'type="prob"' to get the probabilities
    pred_train <- predict(model, newdata=training[, feature], type="prob")[, 2]  # second column for class 1
    pred_test <- predict(model, newdata=test[, feature], type="prob")[, 2]  # second column for class 1
  }
  truth_train <- training[["Label"]]
  truth_test <- test[["Label"]]
  
  trainperf_tree <- performanceMeasures(pred_train, truth_train, "logistic, training")
  testperf_tree <- performanceMeasures(pred_test, truth_test, "logistic, test")
  
  perftable <- rbind(trainperf_tree, testperf_tree)
  pandoc.table(perftable, justify = perf_justify)
}
```

### Full model
```{r}
library(rpart)
library(rpart.plot)
decision_full_model <- rpart(Label ~ ., data = train_data[, c(all_features, "Label")], method = "class",control = rpart.control(cp = 0.01))
rpart.plot(decision_full_model)
```

```{r}
#As we change the label of the previous part to 0, now we just change back to 1
calcAUC <- function(predcol,outcol) {
  perf <- performance(prediction(predcol,outcol==1),'auc')
  as.numeric(perf@y.values)
}
```


```{r}
print(calcAUC(predict(decision_full_model, train_data)[,2], train_data[,"Label"]))
```

```{r}
print(calcAUC(predict(decision_full_model, test_data)[,2], test_data[,"Label"]))
```

```{r}
pretty_perf_table(decision_full_model, train_data, test_data, all_features, "decision_tree")
```

### Selected variables

```{r}
decision_multi_var_model <- rpart(Label ~ ., data = train_data[, c(one_num_var, "Label")], method = "class")
rpart.plot(decision_multi_var_model)
```

```{r}
print(calcAUC(predict(decision_multi_var_model, train_data)[,2], train_data[,"Label"]))
```

```{r}
print(calcAUC(predict(decision_multi_var_model, test_data)[,2], test_data[,"Label"]))
```

```{r}
pretty_perf_table(decision_multi_var_model, train_data, test_data, all_features, "decision_tree")
```
## Logistic Regression
### Full model
```{r}
formula<-paste("Label", paste(all_features, collapse=" + "), sep=" ~ ")
log_full_model<-glm(formula=formula, data=train_data, family=binomial(link="logit"))
train_data$log_full_pred <- predict(log_full_model, newdata=train_data, type="response")
test_data$log_full_pred <- predict(log_full_model, newdata=test_data, type="response")
```

```{r}
pretty_perf_table(log_full_model, train_data, test_data, all_features, "logistic")
```

```{r}
library(knitr)
kable(table(truth=test_data$Label, prediction=test_data$log_full_pred>0.5))
```

### Selected feature
```{r}
formula<-paste("Label", paste(one_num_var, collapse=" + "), sep=" ~ ")
log_multi_var_model<-glm(formula=formula, data=train_data, family=binomial(link="logit"))
train_data$log_multi_var_pred <- predict(log_multi_var_model, newdata=train_data, type="response")
test_data$log_multi_var_pred <- predict(log_multi_var_model, newdata=test_data, type="response")
```

```{r}
pretty_perf_table(log_multi_var_model, train_data, test_data, all_features, "logistic")
```

```{r}
kable(table(truth=test_data$Label, prediction=test_data$log_multi_var_pred>0.5))
```

ROC plot
```{r}
library(ROCit)

# Function to plot ROC curves for both logistic regression and decision tree models
plot_roc <- function(predcol1_test, outcol1_test, predcol1_train, outcol1_train, 
                     predcol2_test, outcol2_test, predcol2_train, outcol2_train) {
  
  # ROC for the first model (Logistic Regression)
  roc_1_test <- rocit(score = predcol1_test, class = outcol1_test == 1)
  roc_1_train <- rocit(score = predcol1_train, class = outcol1_train == 1)
  
  # ROC for the second model (Decision Tree)
  roc_2_test <- rocit(score = predcol2_test, class = outcol2_test == 1)
  roc_2_train <- rocit(score = predcol2_train, class = outcol2_train == 1)
  
  # Plot test data for both models
  plot(roc_1_test, col = c("blue", "green"), lwd = 2, legend = FALSE, YIndex = FALSE, values = TRUE, asp = 1)
  lines(roc_2_test$TPR ~ roc_2_test$FPR, col = c("red", "green"), lwd = 2)
  
  # Plot training data as dashed lines
  lines(roc_1_train$TPR ~ roc_1_train$FPR, col = c("blue", "green"), lwd = 2, lty = 2)
  lines(roc_2_train$TPR ~ roc_2_train$FPR, col = c("red","green"), lwd = 2, lty = 2)
  
  # Add a legend
  legend("bottomright", col = c("blue", "red","green"), 
         legend = c("Logistic Test", "Decision Tree Test", "Null Model", "Logistic Train", "Decision Tree Train"),
         lwd = 2,lty = c(1, 1, 2, 2, 2))
}
```

```{r, fig.asp=1}
# Generate predictions for both models on test and training datasets
# For Logistic Regression Model
pred_log_full_test <- predict(log_full_model, newdata = test_data, type = "response")
pred_log_full_train <- predict(log_full_model, newdata = train_data, type = "response")

# For Decision Tree Model (using probability predictions)
pred_tree_full_test <- predict(decision_full_model, newdata = test_data, type = "prob")[, 2]
pred_tree_full_train <- predict(decision_full_model, newdata = train_data, type = "prob")[, 2]

# Now call the plot function with the appropriate data
plot_roc(pred_log_full_test, test_data$Label, pred_log_full_train, train_data$Label,
         pred_tree_full_test, test_data$Label, pred_tree_full_train, train_data$Label)

```
```{r, fig.asp=1}
# Generate predictions for both models on test and training datasets
# For Logistic Regression Model
pred_log_multi_test <- predict(log_multi_var_model, newdata = test_data, type = "response")
pred_log_multi_train <- predict(log_multi_var_model, newdata = train_data, type = "response")

# For Decision Tree Model (using probability predictions)
pred_tree_multi_test <- predict(decision_multi_var_model, newdata = test_data, type = "prob")[, 2]
pred_tree_multi_train <- predict(decision_multi_var_model, newdata = train_data, type = "prob")[, 2]

# Now call the plot function with the appropriate data
plot_roc(pred_log_multi_test, test_data$Label, pred_log_multi_train, train_data$Label,
         pred_tree_multi_test, test_data$Label, pred_tree_multi_train, train_data$Label)

```



### Clustering
We will now working on clustering our data. For this one, the total_countries_death data will be used. However, the column Income.group will not be included in this dataset, as it works as a label for the data. Therefore, the dataset will only include country with total death records for each of the death cause.

```{r}
regions
```
```{r}
total_regions_death <- regions %>% group_by(Entity) %>% summarise(across(c("Outdoor.air.pollution": "Iron.deficiency"), sum))
total_regions_death
```

```{r}
cluster_data<-total_regions_death
```

```{r}
cluster_data
```


```{r}
#all_variables<-colnames(label_total_countries_death)[-1] #except the country column
scale_data<-scale(cluster_data[, 2:ncol(cluster_data)]) # this will be the data where each column has 0 mean and unit standard deviation
```


```{r}
attr(scale_data, "scaled:scale")
```

```{r}
attr(scale_data, "scaled:center")
```

```{r}
d<-dist(scale_data, method="euclidean")
d
```
```{r}
pfit<-hclust(d,method="ward.D2")
plot(pfit, labels = cluster_data$Entity, main="Cluster Dendogram for Death Causes")
rect.hclust(pfit, k=3)
```
```{r}
plot(as.dendrogram(pfit), xlim=c(1, 25))
```


```{r}
groups <- cutree(pfit, k=3)
groups
```

```{r}
print(dim(project2D))  # Dimensions of project2D
print(length(groups))  # Length of groups (should be the same as number of rows in project2D)
print(length(cluster_data$Entity))  # Length of cluster_data$Country
```


```{r}
princ <- prcomp(scale_data) # Calculate the principal components of scaled_df
nComp <- 2
project2D <- as.data.frame(predict(princ, newdata=scale_data)[,1:nComp])
hclust.project2D <- cbind(project2D, cluster=as.factor(groups), country=cluster_data$Entity)
head(hclust.project2D)
```

```{r}
# finding convex hull
library('grDevices')
find_convex_hull <- function(proj2Ddf, groups) {
  do.call(rbind,lapply(unique(groups), FUN = function(c) {
    f <- subset(proj2Ddf, cluster==c);
    f[chull(f),]
    }))
  }
hclust.hull <- find_convex_hull(hclust.project2D, groups)
```


```{r}
ggplot(hclust.project2D, aes(x=PC1, y=PC2)) +
geom_point(aes(shape=cluster, color=cluster)) +
geom_text(aes(label=country, color=cluster), hjust=0, vjust=1, size=3) +
geom_polygon(data=hclust.hull, aes(group=cluster, fill=as.factor(cluster)),
alpha=0.4, linetype=0) + theme(text=element_text(size=20))
```
```{r}
# Function to return the squared Euclidean distance of two given points x and y
sqr_euDist <- function(x, y) {
  sum((x - y)**2)
}
# Function to calculate WSS of a cluster, represented as a n-by-d matrix
# (where n and d are the numbers of rows and columns of the matrix)
# which contains only points of the cluster.
wss <- function(clustermat) {
  c0 <- colMeans(clustermat)
  sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} ))
}
# Function to calculate the total WSS. Argument `scaled_df`: data frame
# with normalised numerical columns. Argument `labels`: vector containing
# the cluster ID (starting at 1) for each row of the data frame.
wss_total <- function(scaled_df, labels) {
  wss.sum <- 0
  k <- length(unique(labels))
  for (i in 1:k)
    wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
    wss.sum
}
# Function to calculate total sum of squared (TSS) distance of data
# points about the (global) mean. This is the same as WSS when the
# number of clusters (k) is 1.
tss <- function(scaled_df) {
  wss(scaled_df)
}
```

```{r}
# Function to return the CH indices computed using hierarchical
# clustering (function `hclust`) or k-means clustering (`kmeans`)
# for a vector of k values ranging from 1 to kmax.
CH_index <- function(scaled_df, kmax, method="kmeans") {
  if (!(method %in% c("kmeans", "hclust")))
    stop("method must be one of c('kmeans', 'hclust')")
  
  npts <- nrow(scaled_df)
  wss.value <- numeric(kmax) # create a vector of numeric type
  # wss.value[1] stores the WSS value for k=1 (when all the
  # data points form 1 large cluster).
  wss.value[1] <- wss(scaled_df)
  
  if (method == "kmeans") {
    # kmeans
    for (k in 2:kmax) {
      clustering <- kmeans(scaled_df, k, nstart=10, iter.max=100)
      wss.value[k] <- clustering$tot.withinss
    }
  } else {
    # hclust
    d <- dist(scaled_df, method="euclidean")
    pfit <- hclust(d, method="ward.D2")
    for (k in 2:kmax) {
      labels <- cutree(pfit, k=k)
      wss.value[k] <- wss_total(scaled_df, labels)
    }
  }
  bss.value <- tss(scaled_df) - wss.value # this is a vector
  B <- bss.value / (0:(kmax-1)) # also a vector
  W <- wss.value / (npts - 1:kmax) # also a vector
  data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
```

```{r}
library(gridExtra)
# calculate the CH criterion
crit.df <- CH_index(scale_data, 10, method="hclust")
fig1 <- ggplot(crit.df, aes(x=k, y=CH_index)) +
geom_point() + geom_line(colour="red") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="CH index") + theme(text=element_text(size=20))
fig2 <- ggplot(crit.df, aes(x=k, y=WSS), color="blue") +
geom_point() + geom_line(colour="blue") +
scale_x_continuous(breaks=1:10, labels=1:10) +
theme(text=element_text(size=20))
```

```{r}
grid.arrange(fig1, fig2, nrow=1)
```
NOTE: For the Shiny App, put all the ROC plots into it. Need to add all the plots and tables into the shiny app


## Shiny App
For data exploratory, it will be done using Shiny app

```{r}
# Define UI for application that draws different plot
ui <- fluidPage(
    titlePanel("Visualisation of death in country/region during 1990 - 2019"),
    selectInput("plot_type", "Select plot type:", choices = c("Line plot", "Boxplot", "Heat map", "Correlation plot")),
    sidebarLayout(
        sidebarPanel(
            conditionalPanel(
              condition = "input.plot_type == 'Line plot'",
              checkboxInput("compare", "Compare between 2 countries/regions", FALSE),
            ),
            selectInput("region1", "Choose the first country/region:", choices = unique(data$Entity)),
            conditionalPanel(
              condition = "input.compare == true",
              selectInput("region2", "Choose the second country/region:", choices = unique(data$Entity))
            ),
            conditionalPanel(
              condition = "input.plot_type == 'Line plot'",
              selectInput("reason", "Choose death reason:", choices = names(data)[4:31])
            ),
        ),
        mainPanel(            
          plotOutput("plot1", width = "100%", height = "600px"),
            conditionalPanel(
                condition = "input.compare == true && input.plot_type == 'Line plot'",  # Display plot2 only if compare is checked
                plotOutput("plot2", width = "100%", height = "600px"))
        )
    )
)


# Define the server
server <- function(input, output){
    # Render the histograms
    output$plot1 <- renderPlot({
      region_data<-data[data$Entity==input$region1,]
      # Plot selected death reason
      if (input$plot_type == 'Line plot'){
      ggplot(region_data, aes_string(x="Year", y=input$reason, group = "Entity")) +
          geom_line(color="darkorange") + 
          labs(x="Year", y = input$reason) +
          theme(axis.text.x = element_text(angle = 45, hjust = 1))}
      # Plot the boxplot
      else if (input$plot_type == 'Boxplot'){
        data_melt <- melt(region_data, id.vars = "Year", measure.vars = names(data)[4:31], 
                          variable.name = "death_reason", value.name = "value")
        ggplot(data_melt, aes(x = death_reason, y = value, fill = death_reason)) +
          geom_boxplot() +
          labs(x = "Death Reason", y = "Number of Deaths", title = paste("Boxplot of Death Reasons in", input$region1)) +
          theme_minimal() +
          theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "top")+
          guides(fill = guide_legend(ncol = 3))
        }
      
      # Plot the Heat Map for selected country
      else if (input$plot_type == 'Heat map'){
        data_melt <- melt(region_data, id.vars = "Year", measure.vars = names(data)[4:31], variable.name = "death_reason", value.name = "value")
        # Create heat map
        ggplot(data_melt, aes(x = Year, y = death_reason, fill = value)) + 
            geom_tile() + 
            scale_fill_gradient(low = "aliceblue", high = "blue") +
            scale_x_discrete(breaks = levels(region_data$Year)) +
            labs(x = "Year", y = "Death Reason", title = paste("Heat Map of Death Reasons in", input$region1)) +
            theme_minimal() +
            theme(axis.text.x = element_text(angle = 45, hjust = 1))
      }
      else if (input$plot_type == "Correlation plot"){
        # Extract the number of death for each death reasons
        numeric_data <- region_data[,4:31]
         
        # Calculate the correlation for all variables
        correlation <- cor(numeric_data)
        
        # Melt the correlation for visualization
        correlation_melted <- melt(correlation)
        
        # Create the correlation heatmap
        ggplot(correlation_melted, aes(x = Var1, y = Var2, fill = value)) +
          geom_tile() +
          scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0, limit = c(-1, 1), name = "Correlation") +
          theme_minimal() +
          theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
          labs(title = "Correlation Plot of Death Reasons", x = "Variables", y = "Variables")
      }
    })
    output$plot2 <- renderPlot({ 
      if (input$compare == TRUE && input$plot_type == 'Line plot'){
      region_data<-data[data$Entity==input$region2,]
      # Plot all death reasons
      ggplot(region_data, aes_string(x="Year", y=input$reason, group ="Entity")) + geom_line(color="darkolivegreen") + labs(x="Year", y = input$reason)
    }
  })
}

# Run the Shiny app
shinyApp(ui = ui, server = server)
```












