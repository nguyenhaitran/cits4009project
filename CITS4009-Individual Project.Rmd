---
title: "CITS4009-Individual Project"
author: "Henry Tran"
date: "2024-10-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Install all the required packages for data processing, data transformation and data visualization
library(ggplot2)
library(shiny)
library(gridExtra)
library(reshape2)
library(readxl)
library(dplyr)
```

```{r}
# Read the data
path<-url("https://lms.uwa.edu.au/bbcswebdav/pid-3998436-dt-content-rid-47695412_1/courses/CITS4009_SEM-2_2024/Countries%20and%20death%20causes.csv")
```

# Data Exploratory
```{r}
data<-read.csv(path, header=TRUE)
head(data)
nrow(data)
```
The data has 31 columns and 6840 rows. For the column names, they can be seen that the column names have dot-separated between each word.

```{r}
str(data)
```
```{r}
summary(data)
```
For the first 2 columns, "Entity" and "Code" store different country names and codes. Therefore, to set them as a country as a unique value, it would be the best to convert them into factor

```{r}
data$Entity<-as.factor(data$Entity)
data$Code<-as.factor(data$Code)
data$Year<-as.factor(data$Year)

summary(data)
str(data)
```

```{r}
unique(data$Entity)
```


There are 228 unique entities but there are only 206 codes in this dataset. The reason is because there are some countries and regions do not have unique codes.

There are also no missing data

```{r}
total_missing_value = sum(is.na(data))
total_missing_value
```

While with remaining 29 columns, the maximum numbers are extremely larger than other values. The reason is because the dataset did not only record deaths of countries, but it also recorded the total deaths in groups, regions and in the world. Therefore, if we do the data analysis on the whole dataset, it will create skewness problem. The solution for this would be we can seperated data into different groups: countries, regions and world. I have some examples of the single eda below:

```{r}
boxplot(data$High.systolic.blood.pressure)
```
```{r}
eg1<-log1p(data$Outdoor.air.pollution)
boxplot(eg1)
```



For data exploratory, it will be done using Shiny app

```{r}
# Define UI for application that draws different plot
ui <- fluidPage(
    titlePanel("Visualisation of death in country/region during 1990 - 2019"),
    selectInput("plot_type", "Select plot type:", choices = c("Line plot", "Boxplot", "Heat map", "Correlation plot")),
    sidebarLayout(
        sidebarPanel(
            conditionalPanel(
              condition = "input.plot_type == 'Line plot'",
              checkboxInput("compare", "Compare between 2 countries/regions", FALSE),
            ),
            selectInput("region1", "Choose the first country/region:", choices = unique(data$Entity)),
            conditionalPanel(
              condition = "input.compare == true",
              selectInput("region2", "Choose the second country/region:", choices = unique(data$Entity))
            ),
            conditionalPanel(
              condition = "input.plot_type == 'Line plot'",
              selectInput("reason", "Choose death reason:", choices = names(data)[4:31])
            ),
        ),
        mainPanel(            
          plotOutput("plot1", width = "100%", height = "600px"),
            conditionalPanel(
                condition = "input.compare == true && input.plot_type == 'Line plot'",  # Display plot2 only if compare is checked
                plotOutput("plot2", width = "100%", height = "600px"))
        )
    )
)


# Define the server
server <- function(input, output){
    # Render the histograms
    output$plot1 <- renderPlot({
      region_data<-data[data$Entity==input$region1,]
      # Plot selected death reason
      if (input$plot_type == 'Line plot'){
      ggplot(region_data, aes_string(x="Year", y=input$reason, group = "Entity")) +
          geom_line(color="darkorange") + 
          labs(x="Year", y = input$reason) +
          theme(axis.text.x = element_text(angle = 45, hjust = 1))}
      # Plot the boxplot
      else if (input$plot_type == 'Boxplot'){
        data_melt <- melt(region_data, id.vars = "Year", measure.vars = names(data)[4:31], 
                          variable.name = "death_reason", value.name = "value")
        ggplot(data_melt, aes(x = death_reason, y = value, fill = death_reason)) +
          geom_boxplot() +
          labs(x = "Death Reason", y = "Number of Deaths", title = paste("Boxplot of Death Reasons in", input$region1)) +
          theme_minimal() +
          theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "top")+
          guides(fill = guide_legend(ncol = 3))
        }
      
      # Plot the Heat Map for selected country
      else if (input$plot_type == 'Heat map'){
        data_melt <- melt(region_data, id.vars = "Year", measure.vars = names(data)[4:31], variable.name = "death_reason", value.name = "value")
        # Create heat map
        ggplot(data_melt, aes(x = Year, y = death_reason, fill = value)) + 
            geom_tile() + 
            scale_fill_gradient(low = "aliceblue", high = "blue") +
            scale_x_discrete(breaks = levels(region_data$Year)) +
            labs(x = "Year", y = "Death Reason", title = paste("Heat Map of Death Reasons in", input$region1)) +
            theme_minimal() +
            theme(axis.text.x = element_text(angle = 45, hjust = 1))
      }
      else if (input$plot_type == "Correlation plot"){
        # Extract the number of death for each death reasons
        numeric_data <- region_data[,4:31]
         
        # Calculate the correlation for all variables
        correlation <- cor(numeric_data)
        
        # Melt the correlation for visualization
        correlation_melted <- melt(correlation)
        
        # Create the correlation heatmap
        ggplot(correlation_melted, aes(x = Var1, y = Var2, fill = value)) +
          geom_tile() +
          scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0, limit = c(-1, 1), name = "Correlation") +
          theme_minimal() +
          theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
          labs(title = "Correlation Plot of Death Reasons", x = "Variables", y = "Variables")
      }
    })
    output$plot2 <- renderPlot({ 
      if (input$compare == TRUE && input$plot_type == 'Line plot'){
      region_data<-data[data$Entity==input$region2,]
      # Plot all death reasons
      ggplot(region_data, aes_string(x="Year", y=input$reason, group ="Entity")) + geom_line(color="darkolivegreen") + labs(x="Year", y = input$reason)
    }
  })
}

# Run the Shiny app
shinyApp(ui = ui, server = server)
```
The data needs to be transformed, including divided the data into different groups, including countries, regions. Additionally, log-transformation will be applied for data transformation

## Data Transformation

Firstly, the data will be divided into 2 groups: country and region

There are 28 countries and regions in this data. There are few information we can know if the Entity is a region, including:
- Ends with (WB) or (WHO)
- Start with "World"
- G20
And anything other than this will be considered to be countries.
Here is how we will divide the data into groups

```{r}
region_mask <- grepl("(WB)|(WHO)|^World|^G20|^OECD", data$Entity)

regions<-data[region_mask,]
countries<-data[!region_mask,]
```

```{r}
countries
```

```{r}
regions
```

```{r}
```


#Model: Binary classification Sub-Saharan African countries based on death causes
## Data Transformation
For the classification model, the model will classify the Sub-Saharan African countries based on the death causes. The reason to build this model is because the Sub-Saharan African countries could have different death causes compared to countries from other regions.

According to World Bank, there are 48 countries in the low-income groups. Therefore, we will have a column to label them, the country in Sub-Saharan African region will be 1 and 0 for countries from other regions. 

There is a data file of countries in regions from World Bank. This file will be merge with the current data that we are processing

```{r}
#Here is my file path, please change to your filepath to where you downloaded the file
path<-"/Users/henrytran/Documents/UWA_Master-Data-Science/CITS4009/Project/"
```

```{r}
country_groups_file<- paste0(path, "World Bank Country Groups.xlsx")
country_groups_df<-read_excel(country_groups_file, sheet = "List of economies")
head(country_groups_df)
```
The data that we need will include the the Economy, Code and Income group. Therefore, we will keep only 3 columns

```{r}
country_groups_df<-country_groups_df[,c(1,2,3)] #We need the first 3 columns - Economy, Code, Region
country_groups_df
```
Now, we will merge countries dataset with country groups dataset using Code as key, and we keep all the data on the countries dataset
```{r}
countries_merged<-merge(countries, country_groups_df, by.x="Code", by.y="Code", all.x=TRUE)
countries_merged
```
As there are 4 countries do not have Code, including Northen Ireland, Wales, Scotland, and England. They are all in United Kingdom (UK). Now we will double check to see if the total death of UK is the same to the total death of 4 countries
```{r}
countries_merged[countries_merged$Entity=="United Kingdom",]
```


```{r}
total_uk_death <- colSums(countries_merged[countries_merged$Entity=="United Kingdom", 4:31], na.rm = TRUE)
total_uk_death
```

```{r}
four_countries_death <- subset(countries_merged, Entity %in% c("England", "Northern Ireland", "Wales", "Scotland"))
four_countries_death
```
```{r}
total_four_countries_death <- colSums(four_countries_death[, 4:31], na.rm = TRUE)
total_four_countries_death
```
After calculating the total deaths of 4 countries over years, the numbers of death recorded for each reasons are almost the same to the total deaths of UK over years. Therefore, we will drop these 4 countries out of the countries merged dataset.

Now we will calculate the total death of each country for all death causes.
```{r}
countries_merged<-merge(countries, country_groups_df, by.x="Code", by.y="Code")
countries_merged<-countries_merged[,-32]
countries_merged
```
```{r}
# For the data, group the data into entity and region, then calculate the total death of each death causes of each country
total_countries_death <- countries_merged %>% group_by(Entity, Region) %>% summarise(across(c("Outdoor.air.pollution": "Iron.deficiency"), sum))
total_countries_death
```

After aggregating the model, now we will create a label for each country. As mentioned previously, the countries in Sub-Saharan African region will be 1 and 0 for countries from other regions. 

```{r}
total_countries_death$Label<-ifelse(total_countries_death$Region == "Sub-Saharan Africa", 1, 0)
total_countries_death$Label<-as.factor(total_countries_death$Label)
total_countries_death
```


##Building model
- Explain what a null model would look like for classification
- Find all categorical variables and numerical variables
- Expand the dataset with columns built from single variable models
- Plot AUC and select the best performing variables according to AUC

There are 2 models selected for Sub-Saharan African countries classification based on death causes. They are Decision Trees classifier, and Logistic Regression classifier

First of all, the data needs to be divided into 2 groups: training and testing.
```{r}
set.seed(12345)
random_mask<-runif(nrow(total_countries_death))<0.8 # 80% of data used for training
train_data<-total_countries_death[random_mask,]
test_data<-total_countries_death[!random_mask,]
```

```{r}
train_data<-train_data[,3:ncol(train_data)]
train_data
cat("Training dataset size is", dim(train_data))
```
```{r}
test_data<-test_data[,3:ncol(test_data)]
test_data
cat("Testing dataset size is", dim(test_data))
```
```{r}
# we separated the columns, including the responses and all features
responses<- colnames(total_countries_death) %in% c("Entity", "Region", "Label")
all_features<-colnames(total_countries_death)[!responses]
```

### Decision Trees
Single-variate and Multi-Variate models

#### Single-variate model

We will loop through all of the numerical variable to create single variate models with the training data. Then the best single variate model will be selected

Now we will run through all single numerical variables
```{r}
mkPredC<-function(outCol, varCol, appCol){ #training outcomes, training variable and prediction variable
  outCol <- as.vector(outCol)
  varCol <- as.vector(varCol)
  appCol <- as.vector(appCol)
  pOne<-sum(outCol==1)/length(outCol) #how often the outcome is 1 during training
  #print(is.na(varCol))
  #naTab<-table(as.factor(outCol[is.na(varCol)])) 
  #pOneWna<-(naTab/sum(naTab))[pos] #how often the outcome is positive for NA values
  vTab<-table(as.factor(outCol), varCol)
  pOneWv<-(vTab[1,]+ 1.0e-3 *pOne)/(colSums(vTab)+ 1.0e-3) #how often the outcome is 1 with conditioned on levels of training variable
  pred<-pOneWv[appCol] #make prediction by looking up levels of appCol
  #pred[is.na(appCol)]<-pOneWna  #add prediction for NA levels of appCol
  pred[is.na(pred)]<-pOne #add prediction for appCol that were unknown when training
  pred
}

mkPredN<- function(outCol, varCol, appCol){
  varCol <- as.numeric(varCol)  # Ensure it's numeric
  appCol <- as.numeric(appCol)  # Ensure it's numeric
  cuts<-unique(quantile(varCol, probs = seq(0,1,0.1), na.rm=T))
  varC<-cut(varCol, cuts)
  appC<-cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}
```

```{r}
library('ROCR')

calcAUC <- function(predcol,outcol) {
  perf <- performance(prediction(predcol,outcol==1),'auc')
  as.numeric(perf@y.values)
}
```

```{r}
for(var in all_features) {
  pi <- paste('pred', var, sep='')
  train_data[,pi] <- mkPredN(train_data[["Label"]], as.numeric(unlist(train_data[[var]])), as.numeric(unlist(train_data[[var]])))
  test_data[,pi] <- mkPredN(test_data[["Label"]], as.numeric(unlist(test_data[[var]])),as.numeric(unlist(test_data[[var]])))
  aucTrain <- calcAUC(train_data[,pi], train_data[,"Label"])
  #if(aucTrain >= 0.50) {
    aucTest <- calcAUC(test_data[,pi], test_data[,"Label"])
    print(sprintf("%s: trainAUC: %4.3f; testAUC: %4.3f",pi, aucTrain, aucTest))
  #}
}
```
The best single feature selection is based on the largest log likelihood value
```{r}
# Create a function to calculate the log likelihood
logLikelihood<-function(ytrue, ypred, epsilon = 1e-15){
  ypred <- pmin(pmax(ypred, epsilon), 1 - epsilon)
  sum(ifelse(ytrue==1, log(ypred+epsilon), log(1-ypred-epsilon)), na.rm=T)
}
```

We will calculate the Null model on the training dataset
```{r}
logNull<-logLikelihood(train_data[,"Label"], sum(train_data[,"Label"]==1)/nrow(train_data))
cat(logNull)
```

```{r}
one_num_var<-c()
minDrop<-10
for (var in all_features){
  pi<-paste("pred", var, sep='')
  
  devDrop<-2*(logLikelihood(train_data[,"Label"], as.numeric(train_data[[pi]]))-logNull)
  #if (devDrop>=minDrop){
    print(sprintf("%s, deviance reduction: %g", pi, devDrop))
    one_num_var<-c(one_num_var, pi)
  #}
}
```

#### Multi-variate model
```{r}
library(rpart)
library(rpart.plot)
decision_multivar_model <- rpart(Label==1 ~ ., data = train_data[, c(all_features, "Label")], method = "class",control = rpart.control(cp = 0.01))
rpart.plot(decision_multivar_model)
```
```{r}
#predict(decision_multivar_model, train_data, type ="prob")
print(calcAUC(predict(decision_multivar_model, train_data, type ="prob")[,2], train_data[, "Label"]))
```

```{r}
print(calcAUC(predict(decision_multivar_model, train_data)[,2], train_data[,"Label"]))
```

```{r}
print(calcAUC(predict(decision_multivar_model, test_data)[,2], test_data[,"Label"]))
```

### Logistic Regression
Null model and full model

```{r}
sum(train_data$Label==1)/nrow(train_data)
```

```{r}
# Step 1: Find the majority class in the training data
majority_class <- as.numeric(names(sort(table(train_data$Label), decreasing = TRUE)[1]))

# Step 2: Create predictions for the test set based on the majority class
null_model_predictions <- rep(majority_class, nrow(test_data))

# Step 3: Evaluate the null model performance
library(caret)

# Convert to factor to match the true label's type
null_model_predictions <- as.factor(null_model_predictions)

# Create a confusion matrix
null_confusion_matrix <- confusionMatrix(null_model_predictions, test_data$Label)

# View the results
print(null_confusion_matrix)
```

This is the full model

```{r}
#import library caret for logistic linear regression
library(caret)
library(lime)
```

```{r}
log_full_model<-caret::train(x=train_data[all_features], y=train_data$Label, method="glm", family = binomial(link="logit"), metric="Accuracy")
log_full_model_prediction<-predict(log_full_model, test_data[all_features])
explainer<-lime(train_data[all_features], model=as_classifier(log_full_model), bin_continuous=TRUE, n_bins=10)
```

```{r}
confusionMatrix(log_full_model_prediction, test_data$Label)
```

NOTE: this is the manual way of doing the logistic regression model:
```{r}
formula <- paste("Label", paste(all_features, collapse=" + "), sep=" ~ ")
```

```{r}
model_logr <- glm(formula=formula, data=train_data, family=binomial(link="logit"))
## Warning: glm.fit: algorithm did not converge
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
train_data$pred <- predict(model_logr, newdata=train_data, type="response")
test_data$pred <- predict(model_logr, newdata=test_data, type="response")
```

```{r}
library(knitr)
kable(table(truth=test_data$Label, prediction=test_data$pred>0.5))
```



















